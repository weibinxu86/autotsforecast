{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e206d91c",
   "metadata": {},
   "source": [
    "# AutoTSForecast ‚Äî Tutorial\n",
    "\n",
    "**Quick Install:** `pip install autotsforecast`\n",
    "\n",
    "**üìö Documentation:**\n",
    "- **[API Reference](../API_REFERENCE.md)**: Complete parameter documentation\n",
    "- **[Quick Start](../QUICKSTART.md)**: 5-minute getting started guide  \n",
    "- **[README](../README.md)**: Package overview and features\n",
    "\n",
    "## What This Tutorial Covers\n",
    "\n",
    "**Core Features:**\n",
    "1. **AutoForecaster** - Automatic model selection with cross-validation\n",
    "2. **Flexible Covariates** - Use external features (temp, promo, etc.)\n",
    "3. **Hierarchical Reconciliation** - Enforce consistency across series\n",
    "4. **Interpretability** - SHAP and DriverAnalyzer for feature importance\n",
    "5. **Backtesting Module** - Standalone CV evaluation with holdout support\n",
    "\n",
    "**Dataset:**\n",
    "- 3 time series: `total`, `region_a`, `region_b` (where `total = region_a + region_b`)\n",
    "- 2 covariates: `temp`, `promo`\n",
    "- 226 training points, 14 test points\n",
    "- Demonstrates both with and without covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "from autotsforecast import AutoForecaster\n",
    "\n",
    "from autotsforecast.backtesting.validator import BacktestValidator\n",
    "\n",
    "from autotsforecast.hierarchical.reconciliation import HierarchicalReconciler\n",
    "\n",
    "from autotsforecast.interpretability.drivers import DriverAnalyzer\n",
    "\n",
    "from autotsforecast.models.base import LinearForecaster, MovingAverageForecaster, VARForecaster\n",
    "\n",
    "from autotsforecast.models.external import ARIMAForecaster, ETSForecaster, LSTMForecaster, ProphetForecaster, RandomForestForecaster, XGBoostForecaster\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=14\n",
    "\n",
    "n=240\n",
    "\n",
    "idx=pd.date_range(\"2023-01-01\",periods=n,freq=\"D\")\n",
    "\n",
    "time_step=np.arange(n)\n",
    "\n",
    "temp=20+8*np.sin(2*np.pi*time_step/7)+np.random.normal(0,0.8,n)\n",
    "\n",
    "promo=(np.random.rand(n)<0.12).astype(int)\n",
    "\n",
    "promo[-horizon:]=(np.random.rand(horizon)<0.45).astype(int)\n",
    "\n",
    "if promo[-horizon:].sum()==0:\n",
    "\n",
    "    promo[-1]=1\n",
    "\n",
    "X=pd.DataFrame({\"temp\":temp,\"promo\":promo},index=idx)\n",
    "\n",
    "# Make regions harder (noisy) but total smoother: a large shared shock\n",
    "# enters regions with opposite signs and cancels in the total.\n",
    "shared=np.random.normal(0,4.0,n)\n",
    "eps_a=np.random.normal(0,0.8,n)\n",
    "eps_b=np.random.normal(0,0.8,n)\n",
    "\n",
    "region_a=40+0.10*time_step+50.0*X[\"promo\"].values+1.6*X[\"temp\"].values+shared+eps_a\n",
    "\n",
    "region_b=25+7.0*np.sin(2*np.pi*time_step/30)+1.8*X[\"temp\"].values-shared+eps_b\n",
    "\n",
    "total=region_a+region_b\n",
    "\n",
    "y=pd.DataFrame({\"region_a\":region_a,\"region_b\":region_b,\"total\":total},index=idx)\n",
    "\n",
    "y_train,y_test=y.iloc[:-horizon],y.iloc[-horizon:]\n",
    "\n",
    "X_train,X_test=X.iloc[:-horizon],X.iloc[-horizon:]\n",
    "\n",
    "rmse=lambda yt,yp:float(np.sqrt(np.mean((np.asarray(yt)-np.asarray(yp))**2)))\n",
    "\n",
    "mape=lambda yt,yp:float(np.mean(np.abs((np.asarray(yt)-np.asarray(yp))/(np.abs(np.asarray(yt))+1e-9)))*100)\n",
    "\n",
    "print(\"üìä Data Overview:\")\n",
    "print(f\"   Training: {y_train.shape[0]} time steps √ó {y_train.shape[1]} series\")\n",
    "print(f\"   Test: {y_test.shape[0]} time steps √ó {y_test.shape[1]} series\")\n",
    "print(f\"   Covariates: {X_train.shape[1]} features (temp, promo)\")\n",
    "print(f\"   Horizon: {horizon} days ahead\")\n",
    "\n",
    "(y_train.shape,y_test.shape,X_train.shape,X_test.shape,int(X_test[\"promo\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Plot each series\n",
    "axes[0, 0].plot(y.index, y['region_a'], label='region_a', color='steelblue')\n",
    "axes[0, 0].axvline(y_train.index[-1], color='red', linestyle='--', alpha=0.7, label='Train/Test split')\n",
    "axes[0, 0].set_title('Region A', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(y.index, y['region_b'], label='region_b', color='darkorange')\n",
    "axes[0, 1].axvline(y_train.index[-1], color='red', linestyle='--', alpha=0.7, label='Train/Test split')\n",
    "axes[0, 1].set_title('Region B', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(y.index, y['total'], label='total', color='green')\n",
    "axes[1, 0].axvline(y_train.index[-1], color='red', linestyle='--', alpha=0.7, label='Train/Test split')\n",
    "axes[1, 0].set_title('Total (region_a + region_b)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot covariates\n",
    "ax2 = axes[1, 1]\n",
    "ax2.plot(X.index, X['temp'], label='temp', color='purple', alpha=0.7)\n",
    "ax2.axvline(X_train.index[-1], color='red', linestyle='--', alpha=0.7, label='Train/Test split')\n",
    "ax2.set_ylabel('Temperature', color='purple')\n",
    "ax2.tick_params(axis='y', labelcolor='purple')\n",
    "ax2.set_title('Covariates (Exogenous Variables)', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "ax2_right = ax2.twinx()\n",
    "ax2_right.scatter(X.index, X['promo'], label='promo', color='red', alpha=0.5, s=30)\n",
    "ax2_right.set_ylabel('Promo (0/1)', color='red')\n",
    "ax2_right.tick_params(axis='y', labelcolor='red')\n",
    "ax2_right.set_ylim(-0.1, 1.1)\n",
    "\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_right.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: {y.shape[0]} total points\")\n",
    "print(f\"  ‚Ä¢ Training: {y_train.shape[0]} points\")\n",
    "print(f\"  ‚Ä¢ Testing: {y_test.shape[0]} points\")\n",
    "print(f\"  ‚Ä¢ Horizon: {horizon}\")\n",
    "print(f\"\\nNote: Promo events are more frequent in the test period (by design)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6721ef",
   "metadata": {},
   "source": [
    "## 1) AutoForecaster: Automatic Model Selection\n",
    "\n",
    "**How It Works:**\n",
    "- **Training**: Uses your historical data\n",
    "- **Model Selection**: Runs time-respecting CV on training data only (NEVER uses test data!)\n",
    "- **Forecasting**: Generates future predictions\n",
    "\n",
    "**Cross-Validation Setup:**\n",
    "With 226 training points, `cv_splits=3`, `test_size=14`:\n",
    "- **Fold 1**: Train [0:212] ‚Üí Validate [212:226] (most recent data)\n",
    "- **Fold 2**: Train [0:198] ‚Üí Validate [198:212]\n",
    "- **Fold 3**: Train [0:184] ‚Üí Validate [184:198]\n",
    "\n",
    "Each fold respects time ordering - no data leakage! Test data [226:240] is completely isolated for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87714a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data summary\n",
    "print(\"=\"*80)\n",
    "print(\"Dataset Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining points: {len(y_train)}\")\n",
    "print(f\"Test points: {len(y_test)}\")\n",
    "print(f\"Forecast horizon: {horizon}\")\n",
    "print(f\"Number of series: {y_train.shape[1]}\")\n",
    "print(f\"Number of covariates: {X_train.shape[1]}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2dd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how CV splits work (no data leakage - respects time order)\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "total_for_cv = n_train\n",
    "\n",
    "# Show CV folds (validation windows work backwards from end of training data)\n",
    "cv_splits = 3  # Number of CV folds\n",
    "for fold in range(cv_splits):\n",
    "    # Validation windows work backwards from end of training data\n",
    "    # Fold 0: validate on [n_train - 3*horizon : n_train - 2*horizon]\n",
    "    # Fold 1: validate on [n_train - 2*horizon : n_train - 1*horizon]\n",
    "    # Fold 2: validate on [n_train - 1*horizon : n_train]\n",
    "    val_end = total_for_cv - (cv_splits - fold - 1) * horizon\n",
    "    val_start = val_end - horizon\n",
    "    train_end = val_start\n",
    "    \n",
    "    y_offset = fold * 0.3\n",
    "    \n",
    "    # Train portion (expanding window - grows each fold)\n",
    "    ax.barh(y_offset, train_end, left=0, height=0.2, color='steelblue', alpha=0.7)\n",
    "    \n",
    "    # Validation portion (within training data)\n",
    "    ax.barh(y_offset, horizon, left=val_start, height=0.2, color='orange', alpha=0.7)\n",
    "    \n",
    "    ax.text(-5, y_offset, f'Fold {fold+1}', va='center', ha='right', fontsize=9)\n",
    "\n",
    "# Show final holdout test\n",
    "y_offset = cv_splits * 0.3 + 0.2\n",
    "ax.barh(y_offset, n_train, left=0, height=0.2, color='steelblue', alpha=0.9)\n",
    "ax.barh(y_offset, n_test, left=n_train, height=0.2, color='red', alpha=0.7)\n",
    "ax.text(-5, y_offset, 'Final', va='center', ha='right', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-10, n_train + n_test + 10)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.set_xlabel('Time (index)', fontsize=11)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Time Series CV: Expanding Window (No Data Leakage)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "train_patch = mpatches.Patch(color='steelblue', alpha=0.7, label='Train')\n",
    "val_patch = mpatches.Patch(color='orange', alpha=0.7, label='Validation')\n",
    "test_patch = mpatches.Patch(color='red', alpha=0.7, label='Holdout Test')\n",
    "ax.legend(handles=[train_patch, val_patch, test_patch], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìã Cross-Validation Setup:\")\n",
    "print(f\"   ‚Ä¢ Training points: {n_train}, Test points: {n_test}\")\n",
    "print(f\"   ‚Ä¢ CV folds: {cv_splits} (validation windows within training data only)\")\n",
    "print(f\"   ‚Ä¢ Fold 1: train [0:{n_train-3*horizon:3d}], validate [{n_train-3*horizon:3d}:{n_train-2*horizon:3d}]\")\n",
    "print(f\"   ‚Ä¢ Fold 2: train [0:{n_train-2*horizon:3d}], validate [{n_train-2*horizon:3d}:{n_train-horizon:3d}]\")\n",
    "print(f\"   ‚Ä¢ Fold 3: train [0:{n_train-horizon:3d}], validate [{n_train-horizon:3d}:{n_train:3d}]\")\n",
    "print(f\"\\nüîí No data leakage: Test data [{n_train}:{n_train+n_test}] never used for model selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate models\n",
    "cv_splits = 3\n",
    "candidates = [\n",
    "    VARForecaster(horizon=horizon, lags=7),\n",
    "    ETSForecaster(horizon=horizon, seasonal_periods=7, trend=None, seasonal=\"add\"),\n",
    "    ARIMAForecaster(horizon=horizon, order=(1,1,1), seasonal_order=(1,0,1,7)),\n",
    "    MovingAverageForecaster(horizon=horizon, window=7),\n",
    "    LinearForecaster(horizon=horizon),\n",
    "    RandomForestForecaster(horizon=horizon, n_lags=14, n_estimators=400, random_state=0),\n",
    "    XGBoostForecaster(horizon=horizon, n_lags=14, n_estimators=400, random_state=0, max_depth=6, learning_rate=0.05),\n",
    "    ProphetForecaster(horizon=horizon),\n",
    "    LSTMForecaster(horizon=horizon, n_lags=21, hidden_size=32, num_layers=1, dropout=0.0, epochs=5, batch_size=64, learning_rate=0.01, random_state=0),\n",
    "]\n",
    "\n",
    "print(f\"Candidate models: {len(candidates)}\")\n",
    "print(f\"CV configuration: {cv_splits} folds, test_size={horizon}\")\n",
    "\n",
    "# Train AutoForecaster WITH covariates\n",
    "auto = AutoForecaster(\n",
    "    candidate_models=candidates,\n",
    "    metric=\"rmse\",\n",
    "    n_splits=cv_splits,\n",
    "    test_size=horizon,\n",
    "    window_type=\"expanding\",\n",
    "    verbose=False,\n",
    "    per_series_models=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining WITH covariates...\")\n",
    "auto.fit(y_train, X_train)\n",
    "\n",
    "print(\"\\nSelected models:\")\n",
    "for series in y_train.columns:\n",
    "    print(f\"  ‚Ä¢ {series}: {type(auto.best_models_[series]).__name__}\")\n",
    "\n",
    "yhat_auto = auto.forecast(X_test)\n",
    "auto_rmse = {c: rmse(y_test[c], yhat_auto[c]) for c in y_test.columns}\n",
    "auto_mape = {c: mape(y_test[c], yhat_auto[c]) for c in y_test.columns}\n",
    "\n",
    "# Train AutoForecaster WITHOUT covariates\n",
    "auto_noX = AutoForecaster(\n",
    "    candidate_models=candidates,\n",
    "    metric=\"rmse\",\n",
    "    n_splits=cv_splits,\n",
    "    test_size=horizon,\n",
    "    window_type=\"expanding\",\n",
    "    verbose=False,\n",
    "    per_series_models=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining WITHOUT covariates...\")\n",
    "auto_noX.fit(y_train, None)\n",
    "\n",
    "print(\"\\nSelected models:\")\n",
    "for series in y_train.columns:\n",
    "    print(f\"  ‚Ä¢ {series}: {type(auto_noX.best_models_[series]).__name__}\")\n",
    "\n",
    "yhat_auto_noX = auto_noX.forecast(None)\n",
    "auto_noX_rmse = {c: rmse(y_test[c], yhat_auto_noX[c]) for c in y_test.columns}\n",
    "auto_noX_mape = {c: mape(y_test[c], yhat_auto_noX[c]) for c in y_test.columns}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAutoForecaster WITH covariates:\")\n",
    "for series in y_test.columns:\n",
    "    print(f\"  {series:10s}: RMSE={auto_rmse[series]:6.2f}, MAPE={auto_mape[series]:5.2f}%\")\n",
    "\n",
    "print(\"\\nAutoForecaster WITHOUT covariates:\")\n",
    "for series in y_test.columns:\n",
    "    print(f\"  {series:10s}: RMSE={auto_noX_rmse[series]:6.2f}, MAPE={auto_noX_mape[series]:5.2f}%\")\n",
    "\n",
    "# Collect individual model results for comparison\n",
    "rows = []\n",
    "for level in y_test.columns:\n",
    "    rows.append({\"model\": \"AutoForecaster (with covariates)\", \"level\": level, \"rmse\": float(auto_rmse[level]), \"mape\": float(auto_mape[level])})\n",
    "for level in y_test.columns:\n",
    "    rows.append({\"model\": \"AutoForecaster (no covariates)\", \"level\": level, \"rmse\": float(auto_noX_rmse[level]), \"mape\": float(auto_noX_mape[level])})\n",
    "\n",
    "# Compare with individual models\n",
    "var_model = VARForecaster(horizon=horizon, lags=7)\n",
    "try:\n",
    "    var_model.fit(y_train, X_train)\n",
    "    yhat_var = var_model.predict(X_test)\n",
    "    for col in y_train.columns:\n",
    "        rows.append({\n",
    "            \"model\": \"VARForecaster (multivariate)\",\n",
    "            \"level\": col,\n",
    "            \"rmse\": float(rmse(y_test[col], yhat_var[col])),\n",
    "            \"mape\": float(mape(y_test[col], yhat_var[col])),\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"\\nVAR model failed: {e}\")\n",
    "\n",
    "# Test individual models on each series\n",
    "for proto in candidates:\n",
    "    if isinstance(proto, VARForecaster):\n",
    "        continue  # Already handled above\n",
    "        \n",
    "    for col in y_train.columns:\n",
    "        m = copy.deepcopy(proto)\n",
    "        try:\n",
    "            X_tr = X_train if getattr(m, \"supports_covariates\", False) else None\n",
    "            X_te = X_test if getattr(m, \"supports_covariates\", False) else None\n",
    "            m.fit(y_train[[col]], X_tr)\n",
    "            yhat = m.predict(X_te)\n",
    "            rows.append({\n",
    "                \"model\": f\"{m.__class__.__name__}\",\n",
    "                \"level\": col,\n",
    "                \"rmse\": float(rmse(y_test[col], yhat[col])),\n",
    "                \"mape\": float(mape(y_test[col], yhat[col])),\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "comparison_df = results.groupby([\"model\", \"level\"])[[\"rmse\", \"mape\"]].mean().reset_index()\n",
    "comparison_df = comparison_df.sort_values([\"level\", \"rmse\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Comparison by Series\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e79d80",
   "metadata": {},
   "source": [
    "## Complete Workflow Summary\n",
    "\n",
    "**Simple 4-Step Process:**\n",
    "\n",
    "```python\n",
    "# Step 1: Define candidate models\n",
    "candidates = [VARForecaster(...), LinearForecaster(...), XGBoostForecaster(...), ...]\n",
    "\n",
    "# Step 2: Create AutoForecaster\n",
    "auto = AutoForecaster(\n",
    "    candidate_models=candidates,\n",
    "    per_series_models=True,  # Each series gets its own best model\n",
    "    n_jobs=-1                # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Step 3: Fit (with optional covariates)\n",
    "auto.fit(y_train, X_train)  # X_train optional (can be None or dict)\n",
    "\n",
    "# Step 4: Forecast\n",
    "forecasts = auto.forecast(X_test)  # X_test optional (must match training)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- ‚úÖ **Automatic Model Selection**: CV selects best model per series\n",
    "- ‚úÖ **Flexible Covariates**: Pass DataFrame (all series) or dict (per-series)\n",
    "- ‚úÖ **Parallel Processing**: `n_jobs=-1` uses all CPU cores\n",
    "- ‚úÖ **Time-Respecting CV**: No data leakage - training never sees test data\n",
    "- ‚úÖ **Transparent**: Inspect selected models via `auto.best_models_[series]`\n",
    "- ‚úÖ **Model-Agnostic Tools**: DriverAnalyzer and SHAP work with any selected model\n",
    "\n",
    "**Optional: Different covariates per series**\n",
    "```python\n",
    "X_dict = {\n",
    "    'region_a': X_df,      # Use covariates for region_a\n",
    "    'region_b': None,      # No covariates for region_b\n",
    "    'total': X_df          # Use covariates for total\n",
    "}\n",
    "auto.fit(y_train, X_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ce061",
   "metadata": {},
   "source": [
    "## 2) Hierarchical Reconciliation\n",
    "\n",
    "**Purpose**: Enforce consistency across hierarchical series (e.g., `total = region_a + region_b`)\n",
    "\n",
    "**Methods Available:**\n",
    "- `'bottom_up'`: Aggregate from bottom level\n",
    "- `'top_down'`: Disaggregate from top level\n",
    "- `'mint_ols'` (used below): MinT optimal reconciliation - balanced approach\n",
    "\n",
    "Sometimes base forecasts are already coherent. To demonstrate reconciliation, we inject a small inconsistency if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_rec=[\n",
    "    VARForecaster(horizon=horizon,lags=7),\n",
    "    ETSForecaster(horizon=horizon,seasonal_periods=7,trend=None,seasonal=\"add\"),\n",
    "    ARIMAForecaster(horizon=horizon,order=(1,1,1),seasonal_order=(1,0,1,7)),\n",
    "    MovingAverageForecaster(horizon=horizon,window=7),\n",
    "    LinearForecaster(horizon=horizon),\n",
    "    RandomForestForecaster(horizon=horizon,n_lags=14,n_estimators=400,random_state=0),\n",
    "    XGBoostForecaster(horizon=horizon,n_lags=14,n_estimators=400,random_state=0,max_depth=6,learning_rate=0.05),\n",
    "    ProphetForecaster(horizon=horizon),\n",
    "    LSTMForecaster(horizon=horizon,n_lags=21,hidden_size=32,num_layers=1,dropout=0.0,epochs=5,batch_size=64,learning_rate=0.01,random_state=0),\n",
    "]\n",
    "\n",
    "auto_rec=AutoForecaster(candidate_models=candidates_rec,metric=\"rmse\",n_splits=cv_splits,test_size=horizon,window_type=\"expanding\",verbose=False,per_series_models=True,n_jobs=-1)\n",
    "\n",
    "auto_rec.fit(y_train,X_train)\n",
    "\n",
    "yhat_raw=auto_rec.forecast(X_test)\n",
    "\n",
    "# If the base forecasts are already coherent, reconciliation will not change anything.\n",
    "# To demonstrate reconciliation behavior, inject a tiny incoherency in that case.\n",
    "yhat_base=yhat_raw.copy()\n",
    "gap=yhat_base[\"total\"]-(yhat_base[\"region_a\"]+yhat_base[\"region_b\"])\n",
    "if float(np.max(np.abs(gap.values)))<1e-8:\n",
    "    yhat_base[\"total\"]=yhat_base[\"total\"]*1.03\n",
    "\n",
    "tree={\"total\":[\"region_a\",\"region_b\"]}\n",
    "\n",
    "recon=HierarchicalReconciler(yhat_base,tree).reconcile(method=\"ols\")\n",
    "yhat_recon=recon.reconciled_forecasts\n",
    "\n",
    "\n",
    "\n",
    "changed=pd.Series({c:bool(not np.allclose(yhat_base[c].values,yhat_recon[c].values)) for c in [\"region_a\",\"region_b\",\"total\"]})\n",
    "print(\"Reconciliation changed forecasts (base -> reconciled):\")\n",
    "print(changed)\n",
    "\n",
    "rows=[]\n",
    "for level in [\"region_a\",\"region_b\",\"total\"]:\n",
    "    rows.append({\"model\":\"base\",\"level\":level,\"rmse\":rmse(y_test[level],yhat_base[level]),\"mape\":mape(y_test[level],yhat_base[level])})\n",
    "    rows.append({\"model\":\"reconciled_ols\",\"level\":level,\"rmse\":rmse(y_test[level],yhat_recon[level]),\"mape\":mape(y_test[level],yhat_recon[level])})\n",
    "\n",
    "pd.DataFrame(rows).sort_values([\"level\",\"model\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6501e2",
   "metadata": {},
   "source": [
    "## 3) Interpretability: DriverAnalyzer & SHAP\n",
    "\n",
    "**DriverAnalyzer**: Analyzes external covariates (like temp, promo) to understand their impact on forecasts\n",
    "\n",
    "**How It Works Internally:**\n",
    "1. **Model-Agnostic Design**: Automatically detects model type and uses appropriate method\n",
    "   - Linear models ‚Üí Uses coefficient values to measure feature impact\n",
    "   - Tree models (XGBoost, RandomForest) ‚Üí Uses built-in feature importance\n",
    "   - Statistical models (ETS, ARIMA) ‚Üí Gracefully handles (these use patterns, not features)\n",
    "\n",
    "2. **Focus on External Covariates**: Shows how temp, promo, etc. drive predictions\n",
    "   - Filters out internal features (lags) to focus on business-relevant drivers\n",
    "   - Displays feature importance scores for each covariate\n",
    "\n",
    "3. **Simple Usage** - Works automatically with AutoForecaster's selected models:\n",
    "```python\n",
    "# Get the selected model for any series\n",
    "model = auto.best_models_['region_a']\n",
    "\n",
    "# DriverAnalyzer detects model type automatically\n",
    "analyzer = DriverAnalyzer(model=model, feature_names=['temp', 'promo'])\n",
    "importance = analyzer.calculate_feature_importance(X_train, y_train[['region_a']])\n",
    "\n",
    "# Returns importance scores showing which covariates matter most\n",
    "```\n",
    "\n",
    "**SHAP Analysis** - For tree models, provides deeper insights:\n",
    "- Shows feature contribution to each prediction\n",
    "- Identifies which lags and covariates are most important\n",
    "- Visual explanations of model behavior\n",
    "\n",
    "Below we demonstrate DriverAnalyzer focusing on **external covariates only** (temp and promo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4408ef",
   "metadata": {},
   "source": [
    "## 4) Standalone Backtesting Module\n",
    "\n",
    "**BacktestValidator**: Independent CV tool that works with ANY forecasting model\n",
    "\n",
    "**Key Differences from AutoForecaster:**\n",
    "- **AutoForecaster**: Uses backtesting internally for model selection (you don't see CV details)\n",
    "- **BacktestValidator**: Standalone tool for transparent evaluation and comparison\n",
    "\n",
    "**Primary Use Case: Holdout Period for Future-Looking Performance**\n",
    "\n",
    "The most important feature of BacktestValidator is the **holdout_period** parameter, which reserves the last N data points as a completely separate test set for production evaluation.\n",
    "\n",
    "**Recommended Workflow:**\n",
    "```python\n",
    "validator = BacktestValidator(\n",
    "    model=model,\n",
    "    n_splits=3,\n",
    "    test_size=14,\n",
    "    holdout_period=14  # ‚≠ê Reserve last 14 points as final holdout test\n",
    ")\n",
    "\n",
    "# Returns both CV metrics (for model tuning) and holdout metrics (for production estimate)\n",
    "cv_metrics, holdout_metrics = validator.run_with_holdout(y_full, X_full)\n",
    "```\n",
    "\n",
    "**Why Use Holdout?**\n",
    "- ‚úÖ **Production Evaluation**: Simulates real deployment on future unseen data\n",
    "- ‚úÖ **Unbiased Estimate**: Holdout data never used during model training or CV\n",
    "- ‚úÖ **Standard ML Practice**: Matches train/validation/test split convention\n",
    "\n",
    "**How It Works:**\n",
    "- With `holdout_period=14` on 240 points:\n",
    "  - **CV runs on**: [0:226] (training portion)\n",
    "  - **Holdout evaluates on**: [226:240] (completely separate future period)\n",
    "  - Within CV portion, folds work backward to maximize training data:\n",
    "    - Fold 1: Train [0:212] ‚Üí Validate [212:226]\n",
    "    - Fold 2: Train [0:198] ‚Üí Validate [198:212]\n",
    "    - Fold 3: Train [0:184] ‚Üí Validate [184:198]\n",
    "\n",
    "**Use holdout_period to assess real-world future performance!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Standalone Backtesting Examples\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# PRIMARY USE CASE: Holdout Period for Future-Looking Performance Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRIMARY EXAMPLE: Holdout Period for Production Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine train and test data to simulate having historical data\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "\n",
    "print(f\"\\nFull dataset: {len(y_full)} points\")\n",
    "print(f\"  - Training/CV portion: [0:226]\")\n",
    "print(f\"  - Holdout test period: [226:240] (FUTURE-LOOKING)\")\n",
    "print(\"\\n‚≠ê This simulates deploying a model and testing on truly unseen future data\\n\")\n",
    "\n",
    "rf_model = RandomForestForecaster(horizon=horizon, n_lags=14, n_estimators=200, random_state=42)\n",
    "validator = BacktestValidator(\n",
    "    model=rf_model,\n",
    "    n_splits=3,\n",
    "    test_size=horizon,\n",
    "    window_type='expanding',\n",
    "    holdout_period=horizon  # ‚≠ê Reserve last 14 points as holdout for production evaluation\n",
    ")\n",
    "\n",
    "# Run both CV (for model tuning) and holdout (for production estimate)\n",
    "cv_metrics, holdout_metrics = validator.run_with_holdout(y_full[['total']], X_full)\n",
    "\n",
    "print(f\"Cross-Validation Results on Training Data [0:226]:\")\n",
    "print(f\"   (Used for model tuning and hyperparameter selection)\")\n",
    "print(f\"   RMSE={cv_metrics['rmse']:.2f}, MAE={cv_metrics['mae']:.2f}, MAPE={cv_metrics['mape']:.2f}%\")\n",
    "\n",
    "print(f\"\\n‚≠ê HOLDOUT TEST PERFORMANCE [226:240] (FUTURE-LOOKING):\")\n",
    "print(f\"   (This is your production performance estimate!)\")\n",
    "print(f\"   RMSE={holdout_metrics['rmse']:.2f}, MAE={holdout_metrics['mae']:.2f}, MAPE={holdout_metrics['mape']:.2f}%\")\n",
    "\n",
    "print(\"\\nCV fold details (within training portion [0:226]):\")\n",
    "display(validator.get_fold_results()[['fold', 'train_start', 'train_end', 'test_start', 'test_end', 'rmse']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Insights:\")\n",
    "print(\"  ‚úÖ Holdout [226:240] is completely separate from training\")\n",
    "print(\"  ‚úÖ Provides unbiased estimate of future production performance\")\n",
    "print(\"  ‚úÖ CV is for tuning, holdout is for final evaluation\")\n",
    "print(\"  ‚úÖ This matches standard ML workflow: train/validation/test split\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# SECONDARY EXAMPLES: CV-only and Model Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECONDARY EXAMPLE 1: CV-only backtesting (no holdout)\")\n",
    "print(\"=\"*80)\n",
    "print(\"(Use this when you don't have extra data for holdout)\")\n",
    "\n",
    "rf_model2 = RandomForestForecaster(horizon=horizon, n_lags=14, n_estimators=200, random_state=42)\n",
    "validator2 = BacktestValidator(\n",
    "    model=rf_model2,\n",
    "    n_splits=3,\n",
    "    test_size=horizon,\n",
    "    window_type='expanding'\n",
    ")\n",
    "\n",
    "cv_metrics2 = validator2.run(y_train[['total']], X_train)\n",
    "print(f\"CV Results (avg across {validator2.n_splits} folds):\")\n",
    "print(f\"   RMSE={cv_metrics2['rmse']:.2f}, MAE={cv_metrics2['mae']:.2f}, MAPE={cv_metrics2['mape']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECONDARY EXAMPLE 2: Model Comparison with CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_model = XGBoostForecaster(horizon=horizon, n_lags=14, n_estimators=200, \n",
    "                               max_depth=5, learning_rate=0.05, random_state=42)\n",
    "xgb_validator = BacktestValidator(xgb_model, n_splits=3, test_size=horizon, window_type='expanding')\n",
    "xgb_cv = xgb_validator.run(y_train[['region_a']], X_train)\n",
    "\n",
    "rf_model3 = RandomForestForecaster(horizon=horizon, n_lags=14, n_estimators=200, random_state=42)\n",
    "rf_validator = BacktestValidator(rf_model3, n_splits=3, test_size=horizon, window_type='expanding')\n",
    "rf_cv = rf_validator.run(y_train[['region_a']], X_train)\n",
    "\n",
    "print(\"\\nModel Comparison (CV on training data):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoostForecaster', 'RandomForestForecaster'],\n",
    "    'CV_RMSE': [xgb_cv['rmse'], rf_cv['rmse']],\n",
    "    'CV_MAE': [xgb_cv['mae'], rf_cv['mae']],\n",
    "    'CV_MAPE': [xgb_cv['mape'], rf_cv['mape']]\n",
    "})\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea036d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DriverAnalyzer: Focus on External Covariates (temp, promo)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìä This analysis shows how EXTERNAL covariates (temp, promo) impact forecasts\")\n",
    "print(\"   Internal features like lags are excluded to focus on business drivers\\n\")\n",
    "\n",
    "# Demonstrate DriverAnalyzer with AutoForecaster's selected models\n",
    "for series in [\"region_a\", \"region_b\", \"total\"]:\n",
    "    selected_model = auto.best_models_[series]\n",
    "    model_type = type(selected_model).__name__\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Series: {series}\")\n",
    "    print(f\"Selected Model: {model_type}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # DriverAnalyzer automatically detects model type and applies appropriate method\n",
    "    try:\n",
    "        # For linear models: use coefficients\n",
    "        if isinstance(selected_model, LinearForecaster):\n",
    "            # LinearForecaster needs special handling - check if it has covariates\n",
    "            if hasattr(selected_model, 'feature_names') and selected_model.feature_names:\n",
    "                da = DriverAnalyzer(model=selected_model, feature_names=selected_model.feature_names)\n",
    "                importance = da.calculate_feature_importance(X_train, y_train[[series]], method=\"coefficients\")\n",
    "                print(f\"\\n‚úÖ Covariate Importance (via coefficients):\")\n",
    "                print(f\"   How temp and promo impact {series} predictions:\")\n",
    "                display(importance)\n",
    "                print(\"\\n   üìà Interpretation:\")\n",
    "                print(f\"      ‚Ä¢ Larger absolute values = stronger impact on forecasts\")\n",
    "                print(f\"      ‚Ä¢ Positive = increases forecast when covariate increases\")\n",
    "                print(f\"      ‚Ä¢ Negative = decreases forecast when covariate increases\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  {model_type} was fitted without covariates\")\n",
    "                print(f\"   (Cannot analyze covariate importance)\")\n",
    "        \n",
    "        # For tree models: could use feature importance or SHAP\n",
    "        elif hasattr(selected_model, 'models') and hasattr(selected_model, 'n_lags'):\n",
    "            print(f\"\\n‚úÖ {model_type} is a tree-based model with covariate support\")\n",
    "            print(f\"   (Use SHAP analysis below for detailed covariate importance)\")\n",
    "        \n",
    "        # For other models with predict method\n",
    "        elif hasattr(selected_model, 'predict'):\n",
    "            print(f\"\\n‚úÖ {model_type} supports predictions\")\n",
    "            print(f\"   (DriverAnalyzer could run permutation importance to measure covariate impact)\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  {model_type} is a statistical model without explicit covariate features\")\n",
    "            print(f\"   (These models use historical patterns and seasonality)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error analyzing {model_type}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"How DriverAnalyzer Works Internally:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  1Ô∏è‚É£  Detects model type (Linear, Tree, Statistical, etc.)\")\n",
    "print(\"  2Ô∏è‚É£  Selects appropriate method:\")\n",
    "print(\"      ‚Ä¢ Linear ‚Üí Coefficient-based importance\")\n",
    "print(\"      ‚Ä¢ Tree ‚Üí Built-in feature_importances_\")\n",
    "print(\"      ‚Ä¢ Any model ‚Üí Permutation importance (fallback)\")\n",
    "print(\"  3Ô∏è‚É£  Focuses on EXTERNAL covariates (business drivers)\")\n",
    "print(\"  4Ô∏è‚É£  Returns interpretable importance scores\")\n",
    "print(\"\\n  ‚úÖ Users just pass auto.best_models_[series] - DriverAnalyzer handles the rest!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Standalone example: LinearForecaster with explicit covariate analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example: Standalone LinearForecaster - Covariate Impact on region_a\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lin1 = LinearForecaster(horizon=1)\n",
    "lin1.fit(y_train[[\"region_a\"]], X_train)\n",
    "da_standalone = DriverAnalyzer(model=lin1, feature_names=['temp', 'promo'])\n",
    "coef_imp = da_standalone.calculate_feature_importance(X_train, y_train[[\"region_a\"]], method=\"coefficients\")\n",
    "\n",
    "print(\"\\nCovariate Coefficient Importance (region_a):\")\n",
    "display(coef_imp)\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   ‚Ä¢ These coefficients show the linear relationship between covariates and forecasts\")\n",
    "print(\"   ‚Ä¢ 1 unit increase in 'temp' changes forecast by the 'temp' coefficient value\")\n",
    "print(\"   ‚Ä¢ 1 unit increase in 'promo' changes forecast by the 'promo' coefficient value\")\n",
    "\n",
    "# SHAP demonstration - shows covariate importance in tree models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP Analysis: Covariate Importance in XGBoost Model\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìä This analysis uses SHAP to explain how temp and promo affect predictions\")\n",
    "print(\"   We fit an XGBoost model on region_a (which has strong covariate effects)\")\n",
    "print(\"   region_a = 40 + 0.10*time + 50.0*promo + 1.6*temp + noise\\n\")\n",
    "\n",
    "xgb1 = XGBoostForecaster(horizon=1, n_lags=21, n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42)\n",
    "xgb1.fit(y_train[[\"region_a\"]], X_train)\n",
    "\n",
    "# Create lag features + covariates (properly aligned)\n",
    "# For predicting at time t, use: lags from t-1, t-2, ..., t-21 and covariates from time t\n",
    "lag_df = pd.concat([y_train[[\"region_a\"]].shift(l).rename(columns={\"region_a\": f\"region_a_lag{l}\"}) for l in range(1, xgb1.n_lags+1)], axis=1)\n",
    "# Don't shift covariates - use them at current time to predict current target\n",
    "X_train_h = pd.concat([lag_df, X_train], axis=1).dropna()\n",
    "\n",
    "print(f\"XGBoost features: 21 lags + 2 covariates (temp, promo) = 23 total features\")\n",
    "print(f\"Feature matrix shape: {X_train_h.shape}\")\n",
    "print(f\"Sample of covariate values - temp range: [{X_train_h['temp'].min():.2f}, {X_train_h['temp'].max():.2f}], promo sum: {X_train_h['promo'].sum()}\")\n",
    "\n",
    "# Use sample for SHAP computation\n",
    "X_sample = X_train_h.sample(min(100, len(X_train_h)), random_state=42)\n",
    "\n",
    "model = xgb1.models[0][0]\n",
    "explainer = shap.Explainer(model, X_sample)\n",
    "shap_values = explainer(X_sample, check_additivity=False)\n",
    "\n",
    "# Get SHAP values as array\n",
    "shap_array = shap_values.values\n",
    "feature_names = list(X_sample.columns)\n",
    "\n",
    "# Calculate mean absolute SHAP values for ranking\n",
    "mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ Covariate Feature Importance (Excluding Lags):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter to show ONLY external covariates\n",
    "covariate_importance = importance_df[importance_df['feature'].isin(['temp', 'promo'])].copy()\n",
    "covariate_importance = covariate_importance.sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"\\nüìä External Covariate Importance:\")\n",
    "# Display with more decimal places to see precise values\n",
    "covariate_importance_display = covariate_importance.copy()\n",
    "covariate_importance_display['mean_abs_shap'] = covariate_importance_display['mean_abs_shap'].apply(lambda x: f\"{x:.6f}\")\n",
    "display(covariate_importance_display)\n",
    "\n",
    "# Calculate relative importance\n",
    "if len(covariate_importance) == 2:\n",
    "    temp_imp = covariate_importance[covariate_importance['feature'] == 'temp']['mean_abs_shap'].iloc[0]\n",
    "    promo_imp = covariate_importance[covariate_importance['feature'] == 'promo']['mean_abs_shap'].iloc[0]\n",
    "    total_cov_imp = temp_imp + promo_imp\n",
    "    print(f\"\\nüìà Relative Covariate Impact:\")\n",
    "    print(f\"   ‚Ä¢ temp:  {temp_imp/total_cov_imp*100:.1f}% of total covariate effect\")\n",
    "    print(f\"   ‚Ä¢ promo: {promo_imp/total_cov_imp*100:.1f}% of total covariate effect\")\n",
    "\n",
    "# Filter SHAP values to ONLY show covariates (exclude lag features)\n",
    "covariate_indices = [i for i, feat in enumerate(feature_names) if feat in ['temp', 'promo']]\n",
    "X_sample_covariates = X_sample.iloc[:, covariate_indices]\n",
    "shap_values_covariates = shap_values[:, covariate_indices]\n",
    "\n",
    "# Visual: SHAP summary plot for COVARIATES ONLY\n",
    "plt.figure(figsize=(10, 5))\n",
    "shap.summary_plot(shap_values_covariates, X_sample_covariates, show=False)\n",
    "plt.title(\"SHAP Summary - External Covariates ONLY (temp, promo)\\n(Lag features excluded)\", \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visual: Bar chart showing covariate importance ONLY\n",
    "plt.figure(figsize=(10, 5))\n",
    "shap.summary_plot(shap_values_covariates, X_sample_covariates, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Feature Importance - Covariates ONLY\\n(Focus on business drivers: temp and promo)\", \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  ‚úÖ DriverAnalyzer focuses on EXTERNAL covariates (temp, promo)\")\n",
    "print(\"  ‚úÖ Automatically selects appropriate importance method per model type\")\n",
    "print(\"  ‚úÖ SHAP provides detailed feature-level explanations for tree models\")\n",
    "print(\"  ‚úÖ Both tools help understand which business drivers impact forecasts\")\n",
    "print(\"  ‚úÖ Lag features are internal mechanics - covariates are actionable insights\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc149c",
   "metadata": {},
   "source": [
    "## 5) Quick Parameter Reference\n",
    "\n",
    "**For complete documentation, see [API_REFERENCE.md](../API_REFERENCE.md)**\n",
    "\n",
    "### AutoForecaster Parameters\n",
    "\n",
    "| Parameter | Common Values | Description |\n",
    "|-----------|--------------|-------------|\n",
    "| `candidate_models` | List of models | Models to compare |\n",
    "| `metric` | `'rmse'`, `'mae'`, `'mape'` | Selection metric |\n",
    "| `n_splits` | `2-5` | Number of CV folds |\n",
    "| `test_size` | `horizon` | Validation window size |\n",
    "| `per_series_models` | `True`, `False` | Per-series selection |\n",
    "| `n_jobs` | `-1` (all cores) | Parallel processing |\n",
    "\n",
    "### Common Model Parameters\n",
    "\n",
    "**VARForecaster:**\n",
    "- `lags`: 1-21 (past time steps)\n",
    "- `trend`: `'c'`, `'ct'`, `'n'`\n",
    "\n",
    "**RandomForest/XGBoost:**\n",
    "- `n_lags`: 7-30 (lag features)\n",
    "- `n_estimators`: 100-500 (trees)\n",
    "- `max_depth`: 5-10 (tree depth)\n",
    "- `learning_rate`: 0.01-0.3 (XGBoost)\n",
    "\n",
    "**ETSForecaster:**\n",
    "- `seasonal_periods`: 7 (weekly), 12 (monthly)\n",
    "- `trend`/`seasonal`: None, `'add'`, `'mul'`\n",
    "\n",
    "**ARIMAForecaster:**\n",
    "- `order`: (p, d, q) - typically p,q: 1-5, d: 0-2\n",
    "- `seasonal_order`: (P, D, Q, s)\n",
    "\n",
    "**LSTMForecaster:**\n",
    "- `n_lags`: 14-60 (sequence length)\n",
    "- `hidden_size`: 16-128\n",
    "- `epochs`: 5-50\n",
    "\n",
    "### Hierarchical Reconciliation\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `'bottom_up'` | Aggregate from bottom |\n",
    "| `'top_down'` | Disaggregate from top |\n",
    "| `'mint_ols'` | MinT optimal (balanced) |\n",
    "| `'mint_shrink'` | MinT with shrinkage |\n",
    "\n",
    "### DriverAnalyzer Methods\n",
    "\n",
    "- `'coefficients'`: Linear model coefficients\n",
    "- `'permutation'`: Permutation importance\n",
    "- `'shap'`: SHAP values (tree models)\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Complete documentation: [API_REFERENCE.md](../API_REFERENCE.md)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
